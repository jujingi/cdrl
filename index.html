<!DOCTYPE html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <!-- Bootstrap CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
    integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

  <title>CDRL</title>
</head>

<body>
  <div class="container">
    <br>
    <div style="text-align: center;">  
      <h1>CDRL</h1>
      <h3> Unsupervised Change Detection Based on Image Reconstruction Loss </h3>
      <div style="margin-top: 15px;">
        <span style="margin-right: 15px; font-size: 1.3em;"><a href="https://m-niemeyer.github.io/" target="_blank">Michael Niemeyer<sup>1,2,3</sup></a></span>
        <span style="margin-right: 15px; font-size: 1.3em;"><a href="https://jonbarron.info/" target="_blank">Jonathan T. Barron<sup>3</sup></a></span>
        <span style="margin-right: 15px; font-size: 1.3em;"><a href="https://bmild.github.io/" target="_blank">Ben Mildenhall<sup>3</sup></a></span>
      </div>
      <div>
        <span style="margin-right: 15px; font-size: 1.3em;"><a href="https://msajjadi.com/" target="_blank">Mehdi S. M. Sajjadi<sup>3</sup></a></span>
        <span style="margin-right: 15px; font-size: 1.3em;"><a href="http://cvlibs.net/" target="_blank">Andreas Geiger<sup>1,2</sup></a></span>
        <span style="font-size: 1.3em;"><a href="https://research.google/people/107229/" target="_blank">Noha Radwan<sup>3</sup></a></span>
      </div>
      <div style="margin-top: 15px;">
        <span style="margin-right: 20px; font-size: 1.2em;"><sup>1</sup>Max Planck Institute for Intelligent
          Systems</span>
        <span style="margin-right: 20px; font-size: 1.2em;"><sup>2</sup>University of TÃ¼bingen</span>
        <span style="font-size: 1.2em;"><sup>3</sup>Google Research</span>
      </div>
      <div>
        <span style="margin-right: 10px; font-size: 1.3em;">CVPR 2022 (oral)</span>
      </div>
    </div>
    <div class="text-center" style="font-size: 1.5em; margin-top: 25px;">
      <a class="btn btn-primary btn-lg" target="_blank"
        href="https://drive.google.com/file/d/1S_NnmhypZjyMfwqcHg-YbWSSYNWdqqlo/view?usp=sharing" role="button"
        style="margin-right: 10px; margin-bottom: 10px;">Paper</a>
      <a class="btn btn-primary btn-lg" target="_blank"
        href="https://youtu.be/QyyyvA4-Kwc" role="button"
        style="margin-right: 10px; margin-bottom: 10px;">Video</a>
      <a class="btn btn-primary btn-lg" target="_blank"
        href="https://drive.google.com/file/d/15ip8Fvfxp6rNRfBnbJEnFCjIJeFMH4CE/view?usp=sharing" role="button"
        style="margin-right: 10px; margin-bottom: 10px;">Supplementary</a>
      <a class="btn btn-primary btn-lg" target="_blank"
        href="https://arxiv.org/abs/2112.00724" role="button"
        style="margin-right: 10px;  margin-bottom: 10px;">Arxiv</a>
      <a class="btn btn-primary btn-lg" target="_blank"
        href="https://github.com/google-research/google-research/tree/master/regnerf" role="button"
        style="margin-bottom: 10px;">Code</a>
    </div>
    <div class="row">
      <div class="col-md-12 col-sm-12 col-xs-12">
        <div class="embed-responsive embed-responsive-21by9">
          <video controls loop muted autoplay class="embed-responsive-item">
            <source src="gfx/room_videos/n3_text.mp4" type="video/mp4">
          </video>
        </div>
        <p class="text-center" style="font-size: 1.5em;">
          <span style="font-weight: bold;">RegNeRF</span> enables realistic view synthesis from as few as 3 input
          images.
        </p>
      </div>
    </div>
    <div style="margin-top: 30px;">
      <h2 class="text-center">
        Abstract
      </h2>
      <p style="font-style: italic; margin-bottom: 5px;">
        To train the change detector, bi-temporal images taken at different times in the same area are used. However, collecting 
        labeled bi-temporal images is expensive and time consuming. To solve this problem, various unsupervised change detection 
        methods have been proposed, but they still require unlabeled bi-temporal images. In this paper, we propose unsupervised 
        change detection based on image reconstruction loss using only unlabeled single temporal single image. The image reconstruction 
        model is trained to reconstruct the original source image by receiving the source image and the photometrically transformed 
        source image as a pair. During inference, the model receives bitemporal images as the input, and tries to reconstruct one
        of the inputs. The changed region between bi-temporal images shows high reconstruction loss. Our change detector showed 
        significant performance in various change detection benchmark datasets even though only a single temporal single source 
        image was used. The code and trained models will be publicly available for reproducibility.
      </p>
      <p style="font-size: 1.2em; margin-top: 0px;">
        <span style="font-weight: bold;">TL;DR:</span> We regularize unseen views during optimization to enable view
        synthesis from sparse inputs with as few as 3 input images.
      </p>
    </div>
    <div style="margin-top:10px;">
      <h2 class="text-center">
        Video
      </h2>
      <div class="embed-responsive embed-responsive-16by9">
        <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/QyyyvA4-Kwc" allowfullscreen></iframe>
      </div>
    </div>
    <div style="margin-top: 50px;">
      <div class="text-center">
        <h2>
          Method Overview
        </h2>
        <img src="gfx/teaser.svg" width=100% class="img-fluid" alt="Responsive image">
      </div>
      <div style="margin-top: 35px;">
        <p>
          NeRF optimizes the reconstruction loss for a given set of input images (<span class="text-primary">blue
            cameras</span>).
          For sparse inputs, however, this leads to degenerate solutions.
          In this work, we propose to sample unobserved views (<span class="text-danger">red cameras</span>) and <span
            style="font-weight: bold;"> regularize the geometry and appearance of patches</span> rendered from those
          views.
          More specifically, we cast rays through the scene and render patches from unobserved viewpoints for a given
          radiance field f.
          We then regularize appearance by feeding the predicted RGB patches through a trained normalizing flow model
          phi
          and maximizing predicted log-likelihood.
          We regularize geometry by enforcing a smoothness loss on the rendered depth patches.
          Further, we avoid divergence at early stages of optimization by annealing the scene sampling space ofter the first iterations.
          Our approach leads to 3D-consistent representations <span style="font-weight: bold;">even for sparse input
            scenarios with as few as 3 input views</span> from which realistic novel views can be rendered.
        </p>
      </div>
    </div>
    <div style="margin-top:50px;">
      <h2 class="text-center">
        Results
      </h2>
      <h4>View Synthesis from 3 Input Views</h4>
      <p>
        While mip-NeRF leads to degenerate view synthesis and predicted scene geometry, our method enables realistic
        view synthesis from 3 input views.
      </p>
      <div class="row">
        <div class="col-md-12 col-sm-12 col-xs-12 gallery">
          <div class="embed-responsive embed-responsive-21by9">
            <video controls loop muted autoplay class="embed-responsive-item">
              <source src="gfx/room_videos/n3_text.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
      <h4>View Synthesis from 6 Input Views</h4>
      <p>
        For 6 input views, mip-NeRF improves but predicted renderings and the optimized scene geometry still contain
        floating artifacts. Our approach leads to smooth predicted scene geometry and realistic novel views.
      </p>
      <div class="row">
        <div class="col-md-12 col-sm-12 col-xs-12 gallery">
          <div class="embed-responsive embed-responsive-21by9">
            <video controls loop muted autoplay class="embed-responsive-item">
              <source src="gfx/room_videos/n6_text.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
      <h4>View Synthesis from 9 Input Views</h4>
      <p>
        For 9 input views, mip-NeRF and our method both lead to high-quality view synthesis. For mip-NeRF, small
        floating artfiacts for far-away novel views near the table are still visible while our predicted scene geometry
        appears more realistic.
      </p>
      <div class="row">
        <div class="col-md-12 col-sm-12 col-xs-12 gallery">
          <div class="embed-responsive embed-responsive-21by9">
            <video controls loop muted autoplay class="embed-responsive-item">
              <source src="gfx/room_videos/n9_text.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
      <div style="margin-top:50px;">
        <h2 class="text-center">
          More Results
        </h2>
        <!-- <p style="margin-top: 10px;">
          For more results, check out <a href="comparisons.html" target="_blank">the baseline comparisons</a> or <a
            href="ours.html" target="_blank">more view synthesis results from our method</a>.
        </p> -->
        <p>For more results, check out:</p>
        <div class="row">
          <div class="col-md-6 col-sm-6 col-xs-6 gallery">
            <div class="text-center">
              <a href="comparisons.html" target="_blank">
                <h4>Comparison to Baselines</h4>
                <img src="gfx/comparisons/ours/n9trex_thumbnail.gif" alt="Comparison Image" class="rounded">
              </a>
            </div>
          </div>
          <div class="col-md-6 col-sm-6 col-xs-6 gallery">
            <div class="text-center">
              <a href="ours.html" target="_blank">
                <h4>More Results from our Method</h4>
                <img src="gfx/ours/n9_orchids_thumbnail.gif" alt="Ours Image" class="rounded">
              </a>
            </div>
          </div>
        </div>
      </div>
      <div>

        <h2 class="text-center" style="margin-top: 30px;">
          Citation
        </h2>
        <p>
          If you want to cite our work, please use:
        </p>
        <pre>
        @InProceedings{Niemeyer2021Regnerf,
          author    = {Michael Niemeyer and Jonathan T. Barron and Ben Mildenhall and Mehdi S. M. Sajjadi and Andreas Geiger and Noha Radwan},  
          title     = {RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs},
          booktitle = {Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)},
          year      = {2022},
        }
      </pre>
      </div>

      <!-- Optional JavaScript -->
      <!-- jQuery first, then Popper.js, then Bootstrap JS -->
      <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
        integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
        crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js"
        integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q"
        crossorigin="anonymous"></script>
      <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js"
        integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl"
        crossorigin="anonymous"></script>
</body>

</html>
